# Movies-ETL

## Mixing the Datasets
In this challenge, I extracted data from various sources such as Wikipedia and IMDB to extract as much information as possible about movies. With the challenge, I coded with Python and worked hand in hand with the Pandas library throughout the project. The main purpose of this exercise was to work on using ETL methods. Extracting, Transferring, and Loading data to be used. I looked for inconsistancies such as null values, incorrect data types, blantant errors in data entry, and much more. If the information was salvagable, I did everything I could to preserve it. In this module, I cleaned the data across the datasets and spliced the information that was important together. For example, I combined several of the characters from the movies description alongside the ratings data, that wasn't included previously. To finish off this project, I offloaded my findings onto SQL through pgAdmin.

## The Challenges of Cleaning Data
In certain situations, not all data is salvagable. In some cases I worked with, the data was completely missing from a row, had data not related to the same movie, or had data type errors everywhere. From the data analyst perspective, some of these very rare exceptions in the dataset have to be taken with a grain of salt. In some instances, it is not worth making a case for every error if the sample size affected is extremely small (for example, less than 1/1,000,000 of an error). So, looking across the data, I performed the most time and cost effective manner going forward.
